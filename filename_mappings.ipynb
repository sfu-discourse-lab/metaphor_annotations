{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3af32a7-e5ae-4103-adeb-6c3ae59c2dbe",
   "metadata": {},
   "source": [
    "# File ID Equivalencies Across MIP, CMT, and Appraisal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4f328-fd0e-4119-9a8c-4ea6962c47ed",
   "metadata": {},
   "source": [
    "This notebook creates a dataframe that shows how the file names in MIP, Appraisal, the CMT Sample, and the CMT Project map to one another. \n",
    "It also generates source texts for all of the comments in the MIP project and saves them under their MIP names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54b998-82d6-4fd6-b382-22a24a82c767",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8463c3-a059-4fb0-824c-8876fdb0539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8418cfd-ea27-4863-a1b7-0cda2d6c6906",
   "metadata": {},
   "source": [
    "## Reading and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255e23a-1998-4c32-b11d-da0760f2c5fc",
   "metadata": {},
   "source": [
    "### MIP Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a269280a-94c0-401a-9b15-28f34ee9a75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: filename, dtype: object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in metaphor annotation json and save it to a pandas dataframe\n",
    "file_path = 'MIP-at-2025-08-08.json'\n",
    "met_df = pd.read_json(file_path)\n",
    "\n",
    "# extracting labels from the annotations column\n",
    "met_df['labels'] = met_df.apply(lambda row: row.annotations[0]['result'], axis=1)\n",
    "\n",
    "# changing file names to folder IDs\n",
    "met_df['filename'] = met_df.apply(lambda row: re.sub(r\"^[^_]*-\", '', row.file_upload), axis=1)\n",
    "met_df['filename'] = met_df.apply(lambda row: re.sub(r\"_fixed\", '', row.filename[:-4]), axis=1)\n",
    "met_df['filename'] = met_df.apply(lambda row: re.sub(r\"_NEW\", '', row.filename).lower(), axis=1)\n",
    "\n",
    "# adding text column\n",
    "met_df['text'] = met_df.apply(lambda row: row.data['text'], axis=1)\n",
    "\n",
    "# checking for duplicated files\n",
    "met_df.loc[met_df.duplicated(subset=['filename'])].filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4629fd4-93b2-4ed5-9aaf-adb27b8a18fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(met_df.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7a0ba-88cf-49d9-8c99-4ce0135d0f0b",
   "metadata": {},
   "source": [
    "### CMT Sample Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9f960c9-e8e3-4127-8dd8-e0675b77f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in CMT annotation json and save it to a pandas dataframe\n",
    "cmt_file_path = 'CMT_July30.json'\n",
    "cmt_met_df = pd.read_json(cmt_file_path)\n",
    "\n",
    "# extracting labels from the annotations column\n",
    "cmt_met_df['labels'] = cmt_met_df.apply(lambda row: row.annotations[0]['result'], axis=1)\n",
    "\n",
    "# cleaning up file names \n",
    "cmt_met_df['filename'] = cmt_met_df.apply(lambda row: re.sub(r\"^[^_]*-\", '', row.file_upload), axis=1)\n",
    "cmt_met_df['filename'] = cmt_met_df.apply(lambda row: re.sub(r\"_fixed\", '', row.filename[:-4]), axis=1)\n",
    "\n",
    "# adding text column\n",
    "cmt_met_df['text'] = cmt_met_df.apply(lambda row: row.data['text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3cc43-196e-4475-b252-86f65be3b107",
   "metadata": {},
   "source": [
    "### Appraisal Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3640b766-1119-4ddb-b0a3-d8d585012361",
   "metadata": {},
   "outputs": [],
   "source": [
    "appraisal_dir = os.listdir('SOCC/annotated/Appraisal/Appraisal_annotations/curation')\n",
    "# appraisal_files = [file.replace('.txt','').replace('.tsv','') for file in appraisal_dir]\n",
    "appraisal_text_dict = {}\n",
    "for folder in appraisal_dir:\n",
    "    path = 'SOCC/annotated/Appraisal/Appraisal_annotations/curation/' + folder + '/CURATION_USER.tsv' \n",
    "    # reading and saving the source texts from the annotation to a dictionary\n",
    "    with open(path, 'r', encoding=\"utf8\") as file:\n",
    "        lines = file.readlines()\n",
    "        text = ''.join([line for line in lines if line.startswith('#Text')]).replace('#Text=','\"')\n",
    "        appraisal_text_dict[folder.replace('.txt','').replace('.tsv','')] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13159790-9810-4f55-ab32-03be8294d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(appraisal_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675f2ce-f60b-4985-8d86-d7fee388e7e7",
   "metadata": {},
   "source": [
    "## MIP/Appraisal Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "011fc1a0-fa64-42f4-b235-1b57c24628ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary where MIP name is the key and Appraisal name is the value\n",
    "appr_mip_corresp = {}\n",
    "mip_filenames=met_df.filename.unique()\n",
    "for name in mip_filenames:\n",
    "    # text corresponding to given name in MIP\n",
    "    met_text = str(met_df[met_df['filename']==name].reset_index().text[0])\n",
    "    try: \n",
    "        # text corresponding to given name in appraisal \n",
    "        appr_text = appraisal_text_dict[name]\n",
    "        # similarity score\n",
    "        sim_score = fuzz.ratio(met_text, appr_text)\n",
    "    except:\n",
    "        sim_score = 0 # no such filename in appraisal\n",
    "    # if over 80, it's a match! the two are the same\n",
    "    if sim_score >= 80:\n",
    "        appr_mip_corresp[name] = name\n",
    "    # if not, we need to find the corresponding name \n",
    "    else:\n",
    "        # find the topic\n",
    "        topic = re.sub(r\"_\\d+\", '', name)\n",
    "        # find all file names in Appraisal that fall under that specific topic\n",
    "        topic_names_total = [filename for filename in appraisal_text_dict.keys() if topic in filename]\n",
    "        # remove file names that have already been matched\n",
    "        topic_names_used = [filename for filename in appr_mip_corresp.values() if topic in filename]\n",
    "        # create list of file names for a specific topic that have NOT already been matched\n",
    "        topic_names_to_check = set(topic_names_total) - set(topic_names_used)\n",
    "        cand_dict = {}\n",
    "        for candidate in topic_names_to_check:\n",
    "            cand_text = appraisal_text_dict[candidate]\n",
    "            sim_score_cand = fuzz.ratio(met_text, cand_text)\n",
    "            if sim_score_cand >= 80:\n",
    "                # each candidate file name is a key, assign its similarity score as its value if above 80\n",
    "                cand_dict[candidate] = sim_score_cand\n",
    "        if len(cand_dict.keys())==1:\n",
    "            # if there is exactly one suitable candidate, then it's a match!\n",
    "            appr_mip_corresp[name]=list(cand_dict.keys())[0]\n",
    "        else:\n",
    "            print(name)\n",
    "            print(cand_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24d1e7f8-2a80-499a-bec4-76ed37f2e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # duplicate files in MIP\n",
    "# d = appr_mip_corresp\n",
    "# flipped = {}\n",
    "\n",
    "# for key, value in d.items():\n",
    "#     if value not in flipped:\n",
    "#         flipped[value] = [key]\n",
    "#     else:\n",
    "#         flipped[value].append(key)\n",
    "# [(value, key) for key, value in flipped.items() if len(value) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3205df3a-e026-4c28-b0b3-bb00131921ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # files in MIP with no equivalent in Appraisal\n",
    "# set(met_df.filename.unique())-set(appr_mip_corresp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bd0528d-e53c-496e-8299-aec6214318e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # files in Appraisal with no equivalent in MIP\n",
    "# appraisal_files = [file.replace('.txt','').replace('.tsv','') for file in appraisal_dir]\n",
    "# set(appraisal_files)-set(appr_mip_corresp.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06acda3c-d5c5-4c23-a79f-b9631526ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prints the comments in Appraisal with no equivalent in MIP\n",
    "# for x in set(appraisal_files)-set(appr_mip_corresp.values()):\n",
    "#     print(x)\n",
    "#     print(appraisal_text_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98ece0d5-47d3-4d07-bedf-de792d54e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # names in Appraisal but not MIP\n",
    "# set(appraisal_files)-set(mip_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdaf8695-a462-4906-b57a-899158ee5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_appraisal_df=pd.DataFrame(appr_mip_corresp.items(), columns=['MIP', 'Appraisal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d97bf6-e99e-4915-901b-266623bb94a7",
   "metadata": {},
   "source": [
    "## MIP/CMT Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3af718fe-6339-4201-8dad-2e135b45bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary where MIP name is the key and CMT Sample name is the value\n",
    "cmt_mip_corresp = {}\n",
    "for name in cmt_met_df.filename.unique():\n",
    "    # checks whether the last 10 characters match\n",
    "    mip_text=met_df[met_df['filename']==name].reset_index().text[0]\n",
    "    cmt_text = cmt_met_df[cmt_met_df['filename']==name].reset_index().text[0]\n",
    "    sim_score = fuzz.ratio(mip_text, cmt_text)\n",
    "    if sim_score >= 80:\n",
    "        cmt_mip_corresp[name]=name\n",
    "    else:\n",
    "        # find the topic\n",
    "        topic = re.sub(r\"_\\d+\", '', name)\n",
    "        # find all file names in MIP that fall under that specific topic\n",
    "        topic_names_total = [filename for filename in met_df.filename.unique() if topic in filename]\n",
    "        # remove file names that have already been matched\n",
    "        topic_names_used = [filename for filename in cmt_mip_corresp.values() if topic in filename]\n",
    "        # create list of file names for a specific topic that have NOT already been matched\n",
    "        topic_names_to_check = set(topic_names_total) - set(topic_names_used)\n",
    "        cand_dict = {}\n",
    "        for candidate in topic_names_to_check:\n",
    "            cand_text = met_df[met_df['filename']==candidate].reset_index().text[0]\n",
    "            sim_score_cand = fuzz.ratio(cmt_text, cand_text)\n",
    "            if sim_score_cand >= 80:\n",
    "                # each candidate file name is a key, assign its similarity score as its value if above 80\n",
    "                cand_dict[candidate] = sim_score_cand\n",
    "        if len(cand_dict.keys())==1:\n",
    "            # if there is exactly one suitable candidate, then it's a match!\n",
    "            cmt_mip_corresp[list(cand_dict.keys())[0]]=name\n",
    "        else:\n",
    "            print(name)\n",
    "            print(cand_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb795295-cdc3-4113-b09c-ebdedeb44231",
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_cmt_df = pd.DataFrame(cmt_mip_corresp.items(), columns=['MIP', 'CMT Sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a109bb5-76e4-4327-a194-e07700ce94a5",
   "metadata": {},
   "source": [
    "## Central DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab53af78-b65f-4edd-80fd-f7ce0e5014f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the two dataframes to create a more comprehensive central source\n",
    "mapping_df = pd.merge(mip_appraisal_df, mip_cmt_df, on='MIP', how='outer')\n",
    "# creating a CMT Project column, that does not include the file names in the sample, and is equivalent to MIP everywhere else\n",
    "mapping_df['CMT Project']=mapping_df.apply(lambda row: row.MIP if pd.isna(row['CMT Sample']) else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fed56495-3315-415b-ac30-1d793c1d8b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MIP</th>\n",
       "      <th>Appraisal</th>\n",
       "      <th>CMT Sample</th>\n",
       "      <th>CMT Project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aboriginal_1</td>\n",
       "      <td>aboriginal_1</td>\n",
       "      <td>aboriginal_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aboriginal_10</td>\n",
       "      <td>aboriginal_10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aboriginal_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aboriginal_11</td>\n",
       "      <td>aboriginal_11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aboriginal_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aboriginal_12</td>\n",
       "      <td>aboriginal_12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aboriginal_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aboriginal_13</td>\n",
       "      <td>aboriginal_13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aboriginal_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>watch_91</td>\n",
       "      <td>watch_91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>watch_91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>watch_92</td>\n",
       "      <td>watch_92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>watch_92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>watch_93</td>\n",
       "      <td>watch_93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>watch_93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>watch_95</td>\n",
       "      <td>watch_95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>watch_95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>watch_96</td>\n",
       "      <td>watch_89</td>\n",
       "      <td>watch_89</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1043 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MIP      Appraisal    CMT Sample    CMT Project\n",
       "0      aboriginal_1   aboriginal_1  aboriginal_1            NaN\n",
       "1     aboriginal_10  aboriginal_10           NaN  aboriginal_10\n",
       "2     aboriginal_11  aboriginal_11           NaN  aboriginal_11\n",
       "3     aboriginal_12  aboriginal_12           NaN  aboriginal_12\n",
       "4     aboriginal_13  aboriginal_13           NaN  aboriginal_13\n",
       "...             ...            ...           ...            ...\n",
       "1038       watch_91       watch_91           NaN       watch_91\n",
       "1039       watch_92       watch_92           NaN       watch_92\n",
       "1040       watch_93       watch_93           NaN       watch_93\n",
       "1041       watch_95       watch_95           NaN       watch_95\n",
       "1042       watch_96       watch_89      watch_89            NaN\n",
       "\n",
       "[1043 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a279883-c5eb-4ae1-a140-b4a8f00004ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "mapping_df.to_csv('mapping_spreadsheet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351daa55-d1fd-4c4e-a23b-f5f9d5835177",
   "metadata": {},
   "source": [
    "## Generating New Source Comments (based on what is in MIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53e553c3-23d3-440e-bbc7-2b737ad12470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path to save the files in for the CMT Project\n",
    "path_proj = 'SOURCES_FROM_MIP/CMT_PROJECT/'\n",
    "# all the MIP comments minus the ones already annotated in the CMT Sample\n",
    "new_cmt = mapping_df['CMT Project'].dropna().unique()\n",
    "for name in new_cmt:\n",
    "    text = met_df[met_df['filename']==name].reset_index().text[0].replace('“','\"').replace('”','\"').replace('‘',\"'\").replace('’',\"'\").replace('…','...').replace('–','—')\n",
    "    # saving to a .txt file with the corresponding name in MIP\n",
    "    with open(path_proj+name+'.txt', \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3714037-b116-4336-a75d-aa667809272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path to save the files in for the CMT Sample\n",
    "path_sample = 'SOURCES_FROM_MIP/CMT_SAMPLE/'\n",
    "# the MIP comments that were already annotated in the CMT Sample\n",
    "sample_cmt = mapping_df.dropna(axis=0, subset=['CMT Sample']).MIP.unique()\n",
    "for name in sample_cmt:\n",
    "    text = met_df[met_df['filename']==name].reset_index().text[0].replace('“','\"').replace('”','\"').replace('‘',\"'\").replace('’',\"'\").replace('…','...').replace('–','—')\n",
    "    # saving to a .txt file with the corresponding name in MIP\n",
    "    with open(path_sample+name+'.txt', \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d1710-b458-4ec3-8ad0-4ac291dabf08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
